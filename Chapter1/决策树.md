# 决策树

决策树（Decision Tree）是一种非参数的有监督学习方法，它能够从一系列有特征和标签的数据中总结出决策规
则，并用树状图的结构来呈现这些规则，以解决分类和回归问题  。

**决策树算法的核心是要解决两个问题：**  

1）如何从数据表中找出最佳节点和最佳分枝？
2）如何让决策树停止生长，防止过拟合？  



# sklearn中的决策树  

- 模块 `sklearn.tree  `

sklearn  中决策树的类都在 ”tree“ 这个模块之下。这个模块总共包含五个类  

| tree.DecisionTreeClassifier | 分类树                                |
| --------------------------- | ------------------------------------- |
| tree.DecisionTreeRegressor  | 回归树                                |
| tree.export_graphviz        | 将生成的决策树导出为DOT格式，画图专用 |
| tree.ExtraTreeClassifier    | 高随机版本的分类树                    |
| tree.ExtraTreeRegressor     | 高随机版本的回归树                    |

# `DecisionTreeClassifier  `

```python
class sklearn.tree.DecisionTreeClassifier(criterion='gini', 
                                          splitter='best', 
                                          max_depth=None, 
                                          min_samples_split=2, 
                                          min_samples_leaf=1, 
                                          min_weight_fraction_leaf=0.0, 
                                          max_features=None, 
                                          random_state=None, 
                                          max_leaf_nodes=None, 
                                          min_impurity_decrease=0.0, 
                                          min_impurity_split=None, 
                                          class_weight=None, 
                                          presort='deprecated', 
                                          ccp_alpha=0.0
                                         )
```

## 重要参数  

### criterion  

为了要将表格转化为一棵树，决策树需要找出最佳节点和最佳的分枝方法，对分类树来说，衡量这个“最佳”的指标
叫做“不纯度”。通常来说，不纯度越低，决策树对训练集的拟合越好  。

不纯度基于节点来计算，树中的每个节点都会有一个不纯度，并且子节点的不纯度一定是低于父节点的，也就是
说，在同一棵决策树上，叶子节点的不纯度一定是最低的  。

Criterion这个参数正是用来决定不纯度的计算方法的。sklearn提供了两种选择  

1）输入”entropy“，使用信息熵（Entropy )

2）输入”gini“，使用基尼系数（Gini Impurity  )

比起基尼系数，信息熵对不纯度更加敏感，对不纯度的惩罚最强。但是在实际使用中，信息熵和基尼系数的效果基
本相同。信息熵的计算比基尼系数缓慢一些，因为基尼系数的计算不涉及对数。另外，因为信息熵对不纯度更加敏
感，所以信息熵作为指标时，决策树的生长会更加“精细”，因此对于高维数据或者噪音很多的数据，信息熵很容易
过拟合，基尼系数在这种情况下效果往往比较好。当模型拟合程度不足的时候，即当模型在训练集和测试集上都表
现不太好的时候，使用信息熵  。

| 参数                | criterion                                                    |
| ------------------- | ------------------------------------------------------------ |
| 如何影响模型?       | 确定不纯度的计算方法，帮忙找出最佳节点和最佳分枝，不纯度越低，决策树对训练集 的拟合越好 |
| 可能的输入有哪 些？ | 不填默认基尼系数，填写gini使用基尼系数，填写entropy使用信息增益 |
| 怎样选取参数？      | 通常就使用基尼系数 数据维度很大，噪音很大时使用基尼系数 维度低，数据比较清晰的时候，信息熵和基尼系数没区别 当决策树的拟合程度不够的时候，使用信息熵 两个都试试，不好就换另外一个 |

### random_state & splitter  

random_state用来设置分枝中的随机模式的参数，默认None，在高维度时随机性会表现更明显，低维度的数据
（比如鸢尾花数据集），随机性几乎不会显现。输入任意整数，会一直长出同一棵树，让模型稳定下来  

splitter也是用来控制决策树中的随机选项的，有两种输入值，输入”best"，决策树在分枝时虽然随机，但是还是会
优先选择更重要的特征进行分枝（重要性可以通过属性feature_importances_查看），输入“random"，决策树在
分枝时会更加随机，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合。这
也是防止过拟合的一种方式。当你预测到你的模型会过拟合，用这两个参数来帮助你降低树建成之后过拟合的可能
性。当然，树一旦建成，依然是使用剪枝参数来防止过拟合  。

 剪枝参数

在不加限制的情况下，一棵决策树会生长到衡量不纯度的指标最优，或者没有更多的特征可用为止。这样的决策树
往往会过拟合，这就是说，它会在训练集上表现很好，在测试集上却表现糟糕。我们收集的样本数据不可能和整体
的状况完全一致，因此当一棵决策树对训练数据有了过于优秀的解释性，它找出的规则必然包含了训练样本中的噪
声，并使它对未知数据的拟合程度不足  。

为了让决策树有更好的泛化性，我们要对决策树进行剪枝。剪枝策略对决策树的影响巨大，正确的剪枝策略是优化
决策树算法的核心。sklearn为我们提供了不同的剪枝策略  。

### max_depth  

限制树的最大深度，超过设定深度的树枝全部剪掉
这是用得最广泛的剪枝参数，在高维度低样本量时非常有效。决策树多生长一层，对样本量的需求会增加一倍，所
以限制树深度能够有效地限制过拟合。在集成算法中也非常实用。实际使用时，建议从=3开始尝试，看看拟合的效再决定是否增加设定深度  。

### min_samples_leaf & min_samples_split  

min_samples_leaf 限定，一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分
枝就不会发生，或者，分枝会朝着满足每个子节点都包含min_samples_leaf个样本的方向去发生  

一般搭配max_depth使用，在回归树中有神奇的效果，可以让模型变得更加平滑。这个参数的数量设置得太小会引
起过拟合，设置得太大就会阻止模型学习数据。一般来说，建议从=5开始使用。如果叶节点中含有的样本量变化很
大，建议输入浮点数作为样本量的百分比来使用。同时，这个参数可以保证每个叶子的最小尺寸，可以在回归问题
中避免低方差，过拟合的叶子节点出现。对于类别不多的分类问题，=1通常就是最佳选择  。

min_samples_split限定，一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则分枝就不会发生  。

### max_features & min_impurity_decrease  

max_features限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃。和max_depth异曲同工，
max_features是用来限制高维度数据的过拟合的剪枝参数，但其方法比较暴力，是直接限制可以使用的特征数量
而强行使决策树停下的参数，在不知道决策树中的各个特征的重要性的情况下，强行设定这个参数可能会导致模型
学习不足。如果希望通过降维的方式防止过拟合，建议使用PCA，ICA或者特征选择模块中的降维算法。

min_impurity_decrease限制信息增益的大小，信息增益小于设定数值的分枝不会发生。这是在0.19版本中更新的
功能，在0.19版本之前时使用min_impurity_split。  





**目标权重参数**  

### class_weight & min_weight_fraction_leaf  

完成样本标签平衡的参数。样本不平衡是指在一组数据集中，标签的一类天生占有很大的比例。比如说，在银行要
判断“一个办了信用卡的人是否会违约”，就是是vs否（1%：99%）的比例。这种分类状况下，即便模型什么也不
做，全把结果预测成“否”，正确率也能有99%。因此我们要使用 class_weight 参数对样本标签进行一定的均衡，给
少量的标签更多的权重，让模型更偏向少数类，向捕获少数类的方向建模。该参数默认None，此模式表示自动给
与数据集中的所有标签相同的权重。

有了权重之后，样本量就不再是单纯地记录数目，而是受输入的权重影响了，因此这时候剪枝，就需要搭配`min_
weight_fraction_leaf`这个基于权重的剪枝参数来使用。另请注意，基于权重的剪枝参数（例如`min_weight_
fraction_leaf`）将比不知道样本权重的标准（比如min_samples_leaf）更少偏向主导类。如果样本是加权的，则使用基于权重的预修剪标准来更容易优化树结构，这确保叶节点至少包含样本权重的总和的一小部分  

## 重要属性和接口  

### feature_importances_

属性是在模型训练之后，能够调用查看的模型的各种性质。对决策树来说，最重要的是feature_importances_，能够查看各个特征对模型的重要性  

### apply和predict  

sklearn中许多算法的接口都是相似的，比如说我们之前已经用到的fit和score，几乎对每个算法都可以使用。除了
这两个接口之外，决策树最常用的接口还有apply和predict。apply中输入测试集返回每个测试样本所在的叶子节
点的索引，predict输入测试集返回每个测试样本的标签  。

**所有接口中要求输入X_train和X_test的部分，输入的特征矩阵必须至少是一个二维矩阵。sklearn不接受任何一维矩阵作为特征矩阵被输入  。**

<br />

# DecisionTreeRegressor  

```python
class sklearn.tree.DecisionTreeRegressor(criterion='mse', 
                                         splitter='best', 
                                         max_depth=None, 
                                         min_samples_split=2, 
                                         min_samples_leaf=1, 
                                         min_weight_fraction_leaf=0.0, 
                                         max_features=None, 
                                         random_state=None, 
                                         max_leaf_nodes=None, 
                                         min_impurity_decrease=0.0, 
                                         min_impurity_split=None, 
                                         presort='deprecated', 
                                         ccp_alpha=0.0
                                        )
```

几乎所有参数，属性及接口都和分类树一模一样。需要注意的是，在回归树种，没有标签分布是否均衡的问题，因
此没有class_weight这样的参数  。

## 重要参数，属性及接口  

### criterion  

回归树衡量分枝质量的指标，支持的标准有三种  ：

1）输入 "mse" 使用均方误差 mean squared error(MSE)，父节点和叶子节点之间的均方误差的差额将被用来作为特征选择的标准，这种方法通过使用叶子节点的均值来最小化 L2 损失

2）输入 “friedman_mse” 使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差

3）输入 "mae" 使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化 L1 损失

属性中最重要的依然是feature_importances_，接口依然是apply, fit, predict, score最核心。

  

$$

M S E=\frac{1}{N} \sum_{i=1}^{N}\left(f_{i}-y_{i}\right)^{2}
$$



其中N是样本数量，i是每一个数据样本，$$fi$$ 是模型回归出的数值，$$yi$$ 是样本点i实际的数值标签。所以MSE的本质，其实是样本真实数据与回归结果的差异。在回归树中，MSE不只是我们的分枝质量衡量指标，也是我们最常用的衡量回归树回归质量的指标，当我们在使用交叉验证，或者其他方式获取回归树的结果时，我们往往选择均方误差作为我们的评估（在分类树中这个指标是score代表的预测准确率）。在回归中，我们追求的是，MSE越小越好。  

然而，**回归树的接口score返回的是R平方，并不是MSE**。R平方被定义如下：  



$$
R^{2}=1-\frac{u}{v}
$$

$$
u=\sum_{i=1}^{N}\left(f_{i}-y_{i}\right)^{2} \quad v=\sum_{i=1}^{N}\left(y_{i}-\hat{y}\right)^{2}
$$



其中 $$u$$ 是残差平方和（MSE * N），$$v$$ 是总平方和，N 是样本数量，$$i$$ 是每一个数据样本，$$fi$$ 是模型回归出的数值，$$yi$$ 是样本点i实际的数值标签。$$\hat y$$ 是真实数值标签的平均数。R 平方可以为正为负（如果模型的残差平方和远远大于模型的总平方和，模型非常糟糕，R平方就会为负），而均方误差永远为正  

值得一提的是，**虽然均方误差永远为正，但是sklearn当中使用均方误差作为评判标准时，却是计算”负均方误**
**差“（neg_mean_squared_error）**。这是因为sklearn在计算模型评估指标的时候，会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)，因此在sklearn当中，都以负数表示。真正的均方误差MSE的数值，其实就是 neg_mean_squared_error 去掉负号的数字  。

# 决策树的优缺点  

## 决策树优点  

1. 易于理解和解释，因为树木可以画出来被看见
2. 需要很少的数据准备。其他很多算法通常都需要数据规范化，需要创建虚拟变量并删除空值等。但请注意，
   sklearn 中的决策树模块不支持对缺失值的处理。
3. 使用树的成本（比如说，在预测数据的时候）是用于训练树的数据点的数量的对数，相比于其他算法，这是
   一个很低的成本。
4. 能够同时处理数字和分类数据，既可以做回归又可以做分类。其他技术通常专门用于分析仅具有一种变量类
   型的数据集。
5. 能够处理多输出问题，即含有多个标签的问题，注意与一个标签中含有多种标签分类的问题区别开
6. 是一个白盒模型，结果很容易能够被解释。如果在模型中可以观察到给定的情况，则可以通过布尔逻辑轻松
   解释条件。相反，在黑盒模型中（例如，在人工神经网络中），结果可能更难以解释。
7. 可以使用统计测试验证模型，这让我们可以考虑模型的可靠性。
8. 即使其假设在某种程度上违反了生成数据的真实模型，也能够表现良好  

## 决策树的缺点  

1. 决策树学习者可能创建过于复杂的树，这些树不能很好地推广数据。这称为过度拟合。修剪，设置叶节点所
   需的最小样本数或设置树的最大深度等机制是避免此问题所必需的，而这些参数的整合和调整对初学者来说
   会比较晦涩
2. 决策树可能不稳定，数据中微小的变化可能导致生成完全不同的树，这个问题需要通过集成算法来解决。
   决策树的学习是基于贪婪算法，它靠优化局部最优（每个节点的最优）来试图达到整体的最优，但这种做法
   不能保证返回全局最优决策树。这个问题也可以由集成算法来解决，在随机森林中，特征和样本会在分枝过
   程中被随机采样。
3. 有些概念很难学习，因为决策树不容易表达它们，例如XOR，奇偶校验或多路复用器问题。
   如果标签中的某些类占主导地位，决策树学习者会创建偏向主导类的树。因此，建议在拟合决策树之前平衡
   数据集  



# 附录  

## 分类树参数列表  

<center>
    <img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20200113104912.png"/>
</center>



## 分类树属性列表  

<center>
    <img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20200113105315.png"/>
</center>

## 分类树接口列表  

<center>
    <img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20200113105406.png"/>
</center>

