# 随机森林

## 集成算法概述  

集成学习（ensemble learning）是时下非常流行的机器学习算法，它本身不是一个单独的机器学习算法，而是通
过在数据上构建多个模型，集成所有模型的建模结果。基本上所有的机器学习领域都可以看到集成学习的身影，在
现实中集成学习也有相当大的作用，它可以用来做市场营销模拟的建模，统计客户来源，保留和流失，也可用来预
测疾病的风险和病患者的易感性。在现在的各种算法竞赛中，随机森林，梯度提升树（GBDT），Xgboost等集成
算法的身影也随处可见，可见其效果之好，应用之广  

> 集成算法的目标
>
> 集成算法会考虑多个评估器的建模结果，汇总之后得到一个综合的结果，**以此来获取比单个模型更好的回归或 分类表现** .

多个模型集成成为的模型叫做集成评估器（ensemble estimator），组成集成评估器的每个模型都叫做基评估器
（base estimator）。通常来说，有三类集成算法：装袋法（Bagging），提升法（Boosting）和stacking  



<center>
    <img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20200113113258.png"/>
</center>

装袋法的核心思想是构建多个相互独立的评估器，然后对其预测进行平均或多数表决原则来决定集成评估器的结
果。装袋法的代表模型就是随机森林。

提升法中，基评估器是相关的，是按顺序一一构建的。其核心思想是结合弱评估器的力量一次次对难以评估的样本
进行预测，从而构成一个强评估器。提升法的代表模型有 Adaboost 和梯度提升树  

## sklearn中的随机森林

sklearn 中的集成算法模块 ensemble  

|               类                | 类的功能     |
| :-----------------------------: | ------------ |
| ensemble.RandomForestClassifier | 随机森林分类 |
| ensemble.RandomForestRegressor  | 随机森林回归 |

**决策树的核心问题有两个，一个是如何找出正确的特征来进行提问，即如何分枝，二是树生长到什么时候应该停下**

于第一个问题，我们定义了用来衡量分枝质量的指标不纯度，分类树的不纯度用基尼系数或信息熵来衡量，回归
树的不纯度用MSE均方误差来衡量。每次分枝时，决策树对所有的特征进行不纯度计算，选取不纯度最低的特征进行分枝，分枝后，又再对被分枝的不同取值下，计算每个特征的不纯度，继续选取不纯度最低的特征进行分枝。  



# RandomForestClassifier  

```python
class sklearn.ensemble.RandomForestClassifier(n_estimators=100, 
                                              criterion='gini', 
                                              max_depth=None, 
                                              min_samples_split=2, 
                                              min_samples_leaf=1, 
                                              min_weight_fraction_leaf=0.0, 
                                              max_features='auto', 
                                              max_leaf_nodes=None, 
                                              min_impurity_decrease=0.0, 
                                              min_impurity_split=None, 
                                              bootstrap=True, 
                                              oob_score=False, 
                                              n_jobs=None, 
                                              random_state=None, 
                                              verbose=0, 
                                              warm_start=False, 
                                              class_weight=None, 
                                              ccp_alpha=0.0, 
                                              max_samples=None
                                             )
```

## 重要参数  

### 控制基评估器的参数  

| 参数                  | 含义                                                         |
| --------------------- | ------------------------------------------------------------ |
| criterion             | 不纯度的衡量指标，有基尼系数和信息熵两种选择                 |
| max_depth             | 树的最大深度，超过最大深度的树枝都会被剪掉                   |
| min_samples_leaf      | 一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样 本，否则分枝就不会发生 |
| min_samples_split     | 一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分 枝，否则分枝就不会发生 |
| max_features          | max_features限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃， 默认值为总特征个数开平方取整 |
| min_impurity_decrease | 限制信息增益的大小，信息增益小于设定数值的分枝不会发生       |

### n_estimators  

这是森林中树木的数量，即基评估器的数量。这个参数对随机森林模型的精确性影响是单调的，**n_estimators越**
**大，模型的效果往往越好**。但是相应的，任何模型都有决策边界，n_estimators达到一定的程度之后，随机森林的确性往往不在上升或开始波动，并且，n_estimators越大，需要的计算量和内存也越大，训练的时间也会越来越长。对于这个参数，我们是渴望在训练难度和模型效果之间取得平衡  

n_estimators的默认值在现有版本的sklearn中是10，但是在即将更新的0.22版本中，这个默认值会被修正为
100。这个修正显示出了使用者的调参倾向：要更大的n_estimators  。

### random_state  

随机森林的本质是一种装袋集成算法（bagging），装袋集成算法是对基评估器的预测结果进行平均或用多数表决
原则来决定集成评估器的结果  。

建立了25棵树，对任何一个样本而言，平均或多数表决原则下，当且仅当有13棵以上的树判断错误的时候，随机森林才会判断错误。单独一棵决策树对红酒数据集的分类准确率在0.85上下浮动，假设一棵树判断错误的可能性为0.2(ε)，那20棵树以上都判断错误的可能性是  



$$
e_{\text {random}_{\text {forest}}}=\sum_{i=13}^{25} C_{25}^{i} \varepsilon^{i}(1-\varepsilon)^{25-i}=0.000369
$$



其中，$$i$$ 是判断错误的次数，也是判错的树的数量，ε是一棵树判断错误的概率，（1-ε）是判断正确的概率，共判对25-i次  .

随机森林中其实也有random_state，用法和分类树中相似，只不过在分类树中，一个random_state只控制生成一
棵树，而随机森林中的random_state控制的是生成森林的模式，而非让一个森林中只有一棵树  。

当random_state固定时，随机森林中生成是一组固定的树，但每棵树依然是不一致的，这是用”随机挑选特征进行分枝“的方法得到的随机性。并且我们可以证明，当这种随机性越大的时候，袋装法的效果一般会越来越好。用袋装法集成时，基分类器应当是相互独立的，是不相同的  

### bootstrap & oob_score  

要让基分类器尽量都不一样，一种很容易理解的方法是使用不同的训练集来进行训练，而**袋装法正是通过有放回的**
**随机抽样技术来形成不同的训练数据**，bootstrap 就是用来控制抽样技术的参数  

**bootstrap参数默认True，代表采用这种有放回的随机抽样技术**。通常，这个参数不会被我们设置为False  

然而有放回抽样也会有自己的问题。由于是有放回，一些样本可能在同一个自助集中出现多次，而其他一些却可能
被忽略，一般来说，**自助集大约平均会包含63%的原始数据**。因为每一个样本被抽到某个自助集中的概率为  



$$
1-\left(1-\frac{1}{n}\right)^{n}
$$


当n足够大时，这个概率收敛于1-(1/e)，约等于0.632。因此，会有约37%的训练数据被浪费掉，没有参与建模，这些数据被称为袋外数据(out of bag data，简写为oob)。除了我们最开始就划分好的测试集之外，这些数据也可以被用来作为集成算法的测试集。**也就是说，在使用随机森林时，我们可以不划分测试集和训练集，只需要用袋外数据来测试我们的模型即可**  

当然，这也不是绝对的，当n和n_estimators都不够大的时候，很可能就没有数据掉落在袋外，自然也就无法使用 oob 数据来测试模型了  

如果希望用袋外数据来测试，则需要在实例化时就将oob_score这个参数调整为True，训练完毕之后，我们可以用随机森林的另一个重要属性：oob_score_来查看我们的在袋外数据上测试的结果  

## 重要属性和接口  

### estimators_ 

查看森林中树的状况  

```python
rfc = RandomForestClassifier(n_estimators=20,random_state=2)
rfc = rfc.fit(Xtrain, Ytrain)

#随机森林的重要属性之一：estimators，查看森林中树的状况
rfc.estimators_[0].random_state

for i in range(len(rfc.estimators_)):
	print(rfc.estimators_[i].random_state)
```



### oob_score_   

来查看我们的在袋外数据上测试的结果  

```python
#无需划分训练集和测试集
rfc = RandomForestClassifier(n_estimators=25,oob_score=True)
rfc = rfc.fit(wine.data,wine.target)

#重要属性oob_score_
rfc.oob_score_
```



### feature_importances_  

...

### predict_proba

随机森林的接口与决策树完全一致，因此依然有四个常用接口：apply, fit, predict和score。除此之外，还需要注随机森林的predict_proba接口，这个接口返回每个测试样本对应的被分到每一类标签的概率，标签有几个分类就返回几个概率。如果是二分类问题，则predict_proba返回的数值大于0.5的，被分为1，小于0.5的，被分为0。传统的随机森林是利用袋装法中的规则，平均或少数服从多数来决定集成的结果，而sklearn中的随机森林是平均每个样本对应的 predict_proba 返回的概率，得到一个平均概率，从而决定测试样本的分类  

```python
rfc = RandomForestClassifier(n_estimators=25)
rfc = rfc.fit(Xtrain, Ytrain)
rfc.score(Xtest,Ytest)

rfc.feature_importances_
rfc.apply(Xtest)
rfc.predict(Xtest)
rfc.predict_proba(Xtest)
```

<br />

# RandomForestRegressor  

```python
class sklearn.ensemble.RandomForestRegressor(n_estimators=100, 
                                             criterion='mse', 
                                             max_depth=None, 
                                             min_samples_split=2, 
                                             min_samples_leaf=1, 
                                             min_weight_fraction_leaf=0.0, 
                                             max_features='auto', 
                                             max_leaf_nodes=None, 
                                             min_impurity_decrease=0.0, 
                                             min_impurity_split=None, 
                                             bootstrap=True, 
                                             oob_score=False, 
                                             n_jobs=None, 
                                             random_state=None, 
                                             verbose=0, 
                                             warm_start=False, 
                                             ccp_alpha=0.0, 
                                             max_samples=None
                                            )
```

所有的参数，属性与接口，全部和随机森林分类器一致。仅有的不同就是回归树与分类树的不同，不纯度的指标，参数Criterion不一致  。

## 重要参数

### criterion  

1）输入"mse"使用均方误差mean squared error(MSE)，父节点和叶子节点之间的均方误差的差额将被用来作为
特征选择的标准，这种方法通过使用叶子节点的均值来最小化L2损失
2）输入“friedman_mse”使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差
3）输入"mae"使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化L1损失  



$$
M S E=\frac{1}{N} \sum_{i=1}^{N}\left(f_{i}-y_{i}\right)^{2}
$$



其中N是样本数量，$$i$$ 是每一个数据样本，$$f_i$$ 是模型回归出的数值，$$y_i$$ 是样本点i实际的数值标签。所以 MSE 的本质，其实是样本真实数据与回归结果的差异**。在回归树中，MSE 不只是我们的分枝质量衡量指标，也是我们最常用的衡量回归树回归质量的指标**，当我们在使用交叉验证，或者其他方式获取回归树的结果时，我们往往选择均方误差作为我们的评估（在分类树中这个指标是score代表的预测准确率）。在回归中，我们追求的是，MSE越小越好  

然而，**回归树的接口score返回的是R平方，并不是MSE**。R平方被定义如下  



$$
R^{2}=1-\frac{u}{v}
$$

$$
u=\sum_{i=1}^{N}\left(f_{i}-y_{i}\right)^{2} \quad v=\sum_{i=1}^{N}\left(y_{i}-\hat{y}\right)^{2}
$$



其中 $$u$$ 是残差平方和（MSE * N），v是总平方和，N是样本数量，i是每一个数据样本，$$f_i$$ 是模型回归出的数值，$$y_i$$ 是样本点i实际的数值标签。$$\hat y$$ 是真实数值标签的平均数。R 平方可以为正为负（如果模型的残差平方和远远大于模型的总平方和，模型非常糟糕，R平方就会为负），而均方误差永远为正。

值得一提的是，虽然均方误差永远为正，但是sklearn当中使用均方误差作为评判标准时，却是计算”负均方误差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)，因此在sklearn当中，都以负数表示。真正的均方误差MSE的数值，其实就是neg_mean_squared_error去掉负号的数字  

## 重要属性和接口  

重要的属性和接口，都与随机森林的分类器相一致，还是apply, fit, predict和score最为核心。值得一提的是，随机森林回归并没有 predict_proba 这个接口，因为对于回归来说，并不存在一个样本要被分到某个类别的概率问题，因此没有 predict_proba 这个接口  

### n_estimators

```python
from sklearn.datasets import load_boston
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor
boston = load_boston()
regressor = RandomForestRegressor(n_estimators=100,random_state=0)
cross_val_score(regressor, boston.data, boston.target, cv=10
				,scoring = "neg_mean_squared_error")

sorted(sklearn.metrics.SCORERS.keys())
```

返回十次交叉验证的结果，注意在这里，如果不填写scoring = "neg_mean_squared_error"，交叉验证默认的模型衡量指标是R平方，因此交叉验证的结果可能有正也可能有负。而如果写上scoring，则衡量标准是负MSE，交叉验证的结果只可能为负.



<br />

# 附录  

  ## RFC的参数列表  

<center>
    <img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20200113131913.png"/>
</center>

<center>
    <img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20200113131948.png"/>
</center>

<center>
    <img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20200113132021.png"/>
</center>

## RFC的属性列表  

<center>
    <img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20200113132328.png"/>
</center>

## RFC的接口列表  

<center>
    <img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20200113132447.png"/>
</center>



